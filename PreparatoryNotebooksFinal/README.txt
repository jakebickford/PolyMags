These are the notebooks that were used in the second phase of the project, to clean and normalise the text, and experiment with NLP techniques. the Frequency and Topic Modelling notebooks were then used in preparing the public notebooks for phase 3.

Each notebook passes the results to the next one as a pickled dataframe (and sometimes back again when I wanted to do additional processing e.g. adding bigrams to the Lemmataization notebook after doing some Topic Modelling). To create the first dataframe, use 1.DataIngest.ipynb, you will need to download the corpus as .txt files (see https://github.com/jakebickford/PolyMags) and update the relevant path parameters in the notebook. You will also need the filelist.csv (this is the metadata file, that is used to create the initial dataframe) and it is included in this folder, again you will need to update the path parameter in the notebook. Then as you go on, comments in the code should make it clear which notebook creates each given pickle.